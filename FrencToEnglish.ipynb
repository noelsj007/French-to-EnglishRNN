{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ad5c6c8",
   "metadata": {},
   "source": [
    "This will be a bidirectional RNN to translate the french words into English it will be trained to translate.\n",
    "It comes with another word to process the voice and convert it into the text and feeding into the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ceb85f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all libraries\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa9ced8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional,LSTM, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.optimizers.legacy import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e8fd5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.9 (main, Mar  1 2023, 12:20:14) [Clang 14.0.6 ]\n",
      "Pandas 2.0.3\n",
      "Scikit-Learn 1.3.0\n",
      "SciPy 1.11.1\n",
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import tensorflow.keras\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import scipy as sp\n",
    "import tensorflow as tf\n",
    "import platform\n",
    "\n",
    "print(f\"Python {sys.version}\")\n",
    "print(f\"Pandas {pd.__version__}\")\n",
    "print(f\"Scikit-Learn {sk.__version__}\")\n",
    "print(f\"SciPy {sp.__version__}\")\n",
    "gpu = len(tf.config.list_physical_devices('GPU'))>0\n",
    "print(\"GPU is\", \"available\" if gpu else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32162646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English words/sentences</th>\n",
       "      <th>French words/sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Salut!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Cours !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Courez !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Who?</td>\n",
       "      <td>Qui ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wow!</td>\n",
       "      <td>Ça alors !</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  English words/sentences French words/sentences\n",
       "0                     Hi.                 Salut!\n",
       "1                    Run!                Cours !\n",
       "2                    Run!               Courez !\n",
       "3                    Who?                  Qui ?\n",
       "4                    Wow!             Ça alors !"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df  =pd.read_csv('./eng_-french.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "003da571",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng = df['English words/sentences']\n",
    "fr = df['French words/sentences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69b265f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b33fa147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1082098 English words.\n",
      "27393 unique English words.\n",
      "10 Most common words in the English dataset:\n",
      "\"I\" \"to\" \"you\" \"the\" \"a\" \"is\" \"Tom\" \"of\" \"in\" \"have\"\n",
      "\n",
      "1177832 French words.\n",
      "44918 unique French words.\n",
      "10 Most common words in the French dataset:\n",
      "\"de\" \"Je\" \"?\" \"pas\" \"que\" \"à\" \"ne\" \"la\" \"le\" \"Il\"\n"
     ]
    }
   ],
   "source": [
    "english_words_counter = collections.Counter([word for sentence in eng if isinstance(sentence, str) for word in sentence.split()])\n",
    "french_words_counter = collections.Counter([word for sentence in fr if isinstance(sentence, str) for word in sentence.split()])\n",
    "\n",
    "print('{} English words.'.format(len([word for sentence in eng for word in sentence.split()])))\n",
    "print('{} unique English words.'.format(len(english_words_counter)))\n",
    "print('10 Most common words in the English dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*english_words_counter.most_common(10)))[0]) + '\"')\n",
    "print()\n",
    "print('{} French words.'.format(len([word for sentence in fr for word in sentence.split()])))\n",
    "print('{} unique French words.'.format(len(french_words_counter)))\n",
    "print('10 Most common words in the French dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*french_words_counter.most_common(10)))[0]) + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d12f327",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(x):\n",
    "    \n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(x)\n",
    "    return tokenizer.texts_to_sequences(x), tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc002a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(x, length=None):\n",
    "    if length is None:\n",
    "        length = max([len(sentence) for sentence in x])\n",
    "    return pad_sequences(x, maxlen = 55, padding = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24959f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x, y):\n",
    "   \n",
    "    preprocess_x, x_tk = tokenize(x)\n",
    "    preprocess_y, y_tk = tokenize(y)\n",
    "\n",
    "    preprocess_x = pad(preprocess_x)\n",
    "    preprocess_y = pad(preprocess_y)\n",
    "\n",
    "    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n",
    "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
    "\n",
    "    return preprocess_x, preprocess_y, x_tk, y_tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d98fce17",
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer = preprocess(eng, fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a86f4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max English sentence length: 55\n",
      "Max French sentence length: 55\n",
      "English vocabulary size: 14531\n",
      "French vocabulary size: 30660\n"
     ]
    }
   ],
   "source": [
    "max_english_sequence_length = preproc_english_sentences.shape[1]\n",
    "max_french_sequence_length = preproc_french_sentences.shape[1]\n",
    "english_vocab_size = len(english_tokenizer.word_index)\n",
    "french_vocab_size = len(french_tokenizer.word_index)\n",
    "\n",
    "print(\"Max English sentence length:\", max_english_sequence_length)\n",
    "print(\"Max French sentence length:\", max_french_sequence_length)\n",
    "print(\"English vocabulary size:\", english_vocab_size)\n",
    "print(\"French vocabulary size:\", french_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3fad0ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits_to_text(logits, tokenizer):\n",
    "    \n",
    "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    index_to_words[0] = '<PAD>'\n",
    "\n",
    "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aecae54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bd_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \n",
    "    learning_rate = 0.003\n",
    "    \n",
    "    # Build the layers\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(french_vocab_size, 256, input_length=input_shape[1], input_shape=input_shape[1:]))\n",
    "    model.add(Bidirectional(GRU(256, return_sequences=True)))\n",
    "    model.add(TimeDistributed(Dense(1024, activation='relu')))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(TimeDistributed(Dense(english_vocab_size, activation='softmax'))) \n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b85d2e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-20 15:13:41.569495: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2023-07-20 15:13:41.569531: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2023-07-20 15:13:41.569542: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2023-07-20 15:13:41.569762: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-07-20 15:13:41.570919: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 55, 256)           7849216   \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, 55, 512)           789504    \n",
      " al)                                                             \n",
      "                                                                 \n",
      " time_distributed (TimeDist  (None, 55, 1024)          525312    \n",
      " ributed)                                                        \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 55, 1024)          0         \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDi  (None, 55, 14532)         14895300  \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24059332 (91.78 MB)\n",
      "Trainable params: 24059332 (91.78 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Reshape the input\n",
    "tmp_x = pad(preproc_french_sentences, preproc_french_sentences.shape[1])\n",
    "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2]))\n",
    "\n",
    "# Train \n",
    "model = bd_model(\n",
    "    tmp_x.shape,\n",
    "    preproc_english_sentences.shape[1],\n",
    "    len(english_tokenizer.word_index)+1,\n",
    "    len(french_tokenizer.word_index)+1)\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833c3297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2196/2196 [==============================] - 2166s 986ms/step - loss: 1.0492 - accuracy: 0.9348 - val_loss: 2.2991 - val_accuracy: 0.8572\n",
      "Epoch 2/10\n",
      " 332/2196 [===>..........................] - ETA: 27:38 - loss: 1.0586 - accuracy: 0.9342"
     ]
    }
   ],
   "source": [
    "model.fit(tmp_x, preproc_english_sentences, batch_size=64, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04b15fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-21 09:44:46.784847: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-07-21 09:44:46.954896: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-07-21 09:44:46.966319: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 920ms/step\n",
      "French Input: merci\n",
      "Prediction: help you <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have your French input sentence \"merci\" stored in a variable called `french_input_sentence`.\n",
    "french_input_sentence = \"merci\"\n",
    "\n",
    "# 1. Preprocess the French input sentence.\n",
    "preproc_french_input = french_tokenizer.texts_to_sequences([french_input_sentence])\n",
    "preproc_french_input = pad_sequences(preproc_french_input, padding='post')\n",
    "\n",
    "# 2. Pad the preprocessed French input to match the length of English sentences.\n",
    "tmp_french_input = pad(preproc_french_input, preproc_english_sentences.shape[1])\n",
    "\n",
    "# 3. Reshape the padded French input to match the model's input shape.\n",
    "tmp_french_input = tmp_french_input.reshape((-1, preproc_english_sentences.shape[-1]))\n",
    "\n",
    "# 4. Get the prediction using the model.\n",
    "prediction = model.predict(tmp_french_input)[0]\n",
    "predicted_translation = logits_to_text(prediction, english_tokenizer)\n",
    "\n",
    "# 5. Print the prediction.\n",
    "print(\"French Input:\", french_input_sentence)\n",
    "print(\"Prediction:\", predicted_translation)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
